# :rocket: Multimodal Research Transformer

<p align='center>
<img src='assets/multimodal_research.jpg', alt='transformer logo'>
</p>

## 1.Transformer Base :book:

![model architecture](assets/base/model_architecture.jpg)  
Transformer base code comes from the [Attention Is All You Need](https://arxiv.org/pdf/1706.03762v7) paper. It contains Postional Encoding, Multi-Head Attention, Feed Forward Network, Layer Normalization, Dropout, and Residual Connection modules. And the code is in the [models' folder](/base/models) you can read.  
