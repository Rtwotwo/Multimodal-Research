# :rocket: Multimodal Research Transformer

![model architecture](assets/multimodal_research.jpg)

## 1.Mamba Base :book:

![model architecture](base/assets/overall.jpg)  
(Overview.) Structured SSMs independently map each channel (e.g. ğ· = 5) of an input ğ‘¥ to output ğ‘¦ through a higher
dimensional latent state â„ (e.g. ğ‘ = 4). Prior SSMs avoid materializing this large effective state (ğ·ğ‘ , times batch size ğµ and sequence
length ğ¿) through clever alternate computation paths requiring time-invariance: the (Î”, ğ‘¨, ğ‘©, ğ‘ª) parameters are constant across time. Our
selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize
the expanded states in more efficient levels of the GPU memory hierarchy.  
