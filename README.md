# :rocket: Multimodal Research Transformer

![model architecture](assets/multimodal_research.jpg)

## 1.Transformer Base :bulb:

![model architecture](assets/base/model_architecture.jpg)  
Transformer base code comes from the [Attention Is All You Need](https://arxiv.org/pdf/1706.03762v7) paper. It contains Postional Encoding, Multi-Head Attention, Feed Forward Network, Layer Normalization, Dropout, and Residual Connection modules. And the code is in the [models' folder](transformer/base/models) you can read.  
